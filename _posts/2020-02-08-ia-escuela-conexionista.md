---
layout: post
title: "Inteligencia artificial: la escuela conexionista"
date: 2020-02-08
authors: "Santiago López Gagliano"
excerpt: "Redes neuronales, big data y la actualidad de la inteligencia artificial"
tags: [ia]
comments: true
---
Estamos viviendo en un momento importante en la historia inteligencia artificial. No son pocos los expertos en el área que afirman que desde hace unos años estamos presenciando un nuevo boom. La década anterior nos mantuvo en una constante alza de nuevos desarrollos, aplicaciones y perfeccionamientos de técnicas computacionales preexistentes que hacen realidad algunas prácticas que antes eran soñadas y, aunque es común que nuestra capacidad de asombro se vaya difuminando a medida que estas tecnologías ingresan en nuestras vidas cotidianas o en los portales de noticias que consumimos, solo basta con detenernos un momento y observar: el reconocimiento de audio y lenguaje natural nos permiten tener diálogos con bots que, por instantes, pueden darnos una ligera sensación de retórica humana. La implementación algorítmica del deep learning, sumado al acceso de enormes bases de datos, habilita a grandes corporaciones a tener la capacidad de cómputo para analizar y predecir comportamientos masivos. Los algoritmos de reconocimiento facial nos ponen al filo de prácticas de poder que pueden hacer tambalear algunos de los preceptos más básicos de la idea de privacidad y anonimato tal como la conocemos. La posibilidad de realizar diagnósticos médicos complejos o de diseñar de drogas de modo automático, nos obligan a redefinir a la medicina. Estos son sólo algunos de los ejemplos que se nos pueden ocurrir. Todas estas aplicaciones de Inteligencia Artificial tienen un denominador común: la aplicación de redes neuronales artificiales, comprendidas dentro del paradigma conexionista.

Para comprender un poco de qué va esta idea, debemos remontarnos una vez al origen de esta disciplina y visitar a un personaje fundamental para el siglo XX: Alan Turing. Ente sus valiosos aportes se encuentra la delimitación de aquellas funciones que puede procesar una computadora en lo que se dió a conocer como la tesis Turing-Church. De allí, hubo sólo unos pocos pasos para instalar la noción de que la forma de cognición humana era computacional y, por tanto, una computadora podría imitarla. Para esta época, ya se había establecido como un estándar en la neurofisiología que el cerebro está compuesto por unidades llamadas neuronas tal como había descrito Ramón y Cajal en la última década del siglo XIX, y que éstas interactúan entre sí mediante mecanismos de señalización eléctricos explicados en el modelo de Sherrington. Teniendo en cuenta todo esto, los científicos McCulloch y Pitts elaboraron en 1943 uno de los principios para modelar algunas de las funcionalidades abstractas de una neurona. Muy resumidamente, la idea consistía en entender a estas últimas como unidades lógicas que emiten un output una vez que la suma de ciertos inputs supera un valor determinado. Mas de diez años después, Frank Rosenblatt diseñó una implementación computacional llamada Perceptron que, dándole una vuelta de tuerca a la función McCulloch-Pitts, compuso un conjunto de unidades lógicas enlazadas en una red, que permitían representar algunos aspectos de la capacidad de aprendizaje. Esto se hacía a partir de la asignación arbitraria de valores numéricos a cada neurona para posteriormente hacer que realicen predicciones. En función del acierto o desacierto de estas predicciones, se reasigna el valor de las conexiones entre las neuronas, dándole más peso a las neuronas que tenían mayor importancia en la producción del resultado deseado, y otorgándole menor peso a las que tenían menor importancia. Esta red neuronal influenció a muchos desarrollos posteriores no sólo por sus posibilidades, si no también por sus limitaciones, especialmente  por su incapacidad de resolver problemas no lineales o que demanden recursividad, como diferenciar entre una C y una T rotadas, o diferenciar entre un número par y otro impar. Posteriores diseños de redes neuronales artificiales persiguieron el objetivo de solucionar estos problemas. Muchas de ellas lo lograron, y entre los 70 y los 80, se comenzaron a diseñar una cantidad innumerable de estas arquitecturas que poco a poco empezaron a darle forma a las redes neuronales artificiales o redes conexionistas.

Si la vieja escuela consistía en la representación explícita del conocimiento, y la nueva escuela buscaba reconstruir los comportamientos inteligentes más básicos, el conexionismo persigue el objetivo de reconstruir los principios funcionales mediante los cuales opera el sistema nervioso central y poder representar de modo discreto sus procesos. Una consideración fundamental en este proyecto tiene su eje puesto en que las capacidades que estamos acostumbrados a considerar como formas de conocimiento, no se encuentran almacenadas en ciertos estados de un sistema, sino que más bien radican en la conexión global entre estos estados y las reglas que rigen entre dichas conexiones, que se van articulando mediante la experiencia. De esta manera, la premisa es la de abstraer las propiedades funcionales del cerebro e implementarlas en programas en una computadora. Los elementos que componen un sistema de este estilo pueden listarse en los siguientes ítems: un conjunto de unidades de procesamiento, un estado de activación (definido sobre las unidades de procesamiento), una función de output, un patrón de conectividad, una regla de activación, una regla de aprendizaje (que habilita a que los patrones de conectividad se modifiquen mediante la experiencia) y un entorno en el cual el sistema funciona. Las posibilidades que fueron demostrando estos sistemas entusiasmaron a algunos representantes de este área de investigación, que vieron en las redes neuronales la oportunidad de ir ascendiendo peldaño a peldaño en la construcción de sistemas artificiales que pudieran alcanzar este grado de complejidad y de operatividad de un sistema nervioso. Pero lo cierto es que no hizo y no hace falta que esto llegue a lograrse para que las redes neuronales den resultados interesantes o deseables en su aplicación. En este sentido, hablar de una inteligencia artificial propiamente dicha - sin negar la posibilidad de que eso exista o pueda existir - puede entenderse más bien como una mera metáfora.

De esta breve historia de la inteligencia artificial que te estuvimos contando, es quizás ésta la más relevante para comprender muchos de los interesantes avances que vemos semana a semana, a los que tan difícil resulta seguirles el ritmo y que cada vez influencian más en casi todos los aspectos de nuestras vidas.

---
##### La nota que acabás de leer fue escrita por Santiago López Gagliano, continuando con nuestra edición especial de verano sobre la historia de la inteligencia artificial. A continuación, te dejamos algunas referencias para ampliar la lectura de la nota:


##### Haugeland, J (1998) [Mind Design II: Philosophy, Psychology, Artificial Intelligence](https://mitpress.mit.edu/books/mind-design-ii). MIT Press.

##### Una colección de algunos de los papers más importantes de la historia de la inteligencia artificial, con un especial hincapié en el conexionismo.


##### Rumelhart, D.E., J.L. McClelland (1986) [Parallel Distributed Processing: Explorations in the Microstructure of Cognition](https://dl.acm.org/doi/book/10.5555/104279). Volume 1: Foundations. MIT Press.

##### Obra capital para comprender al conexionismo en todos sus aspectos, tanto desde la perspectiva de la inteligencia artificial, como en el resto de las aplicaciones que ha tenido en las ciencias cognitivas en general.
